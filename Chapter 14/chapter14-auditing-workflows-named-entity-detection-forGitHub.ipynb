{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing workflow for named entity detection using Amazon Textract, Amazon Comprehend and Amazon A2I\n",
    "\n",
    "**Note:** This is the accompanying notebook for Chapter 14 - auditing workflows for named entity detection. Before executing the code in this notebook, please review Chapter 14 in the book and the Read Me included along with this notebook. To demonstrate accessing APIs and to see how the solution works step by step we use this Jupyter notebook in the book. You can however build this whole solution using AWS Lambda with event triggers that are alerted whenever a task is completed (for example, you can use the start_entities_detection_job API and setup a Amazon CloudWatch event rule to be triggered when the job is complete, which can execute an AWS Lambda function to perform the next set of steps. Please refer to the Further Reading section in the book to refer to a github repository of this solution deployed using AWS CloudFormation and AWS Lambda. \n",
    "\n",
    "In this notebook we will walk you through the code required to setup your own document processing workflow with Amazon Textract, an Amazon Comprehend Custom Entity Recognizer, and Amazon Augmented AI to extract the content of a sample loan form, detect custom entities to determine if the application should be approved or rejected, setup and send to a human review loop to review predictions, update the entity list, retrain the Comprehend custom entity model, and finally save the loan approval decision to a DynamoDB table.\n",
    "\n",
    "* Step 0 - Install and import libraries\n",
    "* Step 1 - Train an Amazon Comprehend Custom Entity Recognizer\n",
    "* Step 2 - Create a private human review workforce\n",
    "* Step 3 - Extract input document contents using Amazon Textract\n",
    "* Step 4 - Detect custom entities using Amazon Comprehend\n",
    "* Step 5 - Setup and send to Amazon A2I human loop\n",
    "* Step 6 - Review and modify predictions\n",
    "* Step 7 - Retrain Comprehend Custom Entity Recognizer with updated entities\n",
    "* Step 8 - Store predictions for downstream processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Please make sure that you review and complete all the prerequisites documented in Chapter 14 of the book before you execute the code provided in this notebook. To run this notebook you need to ensure that you setup the permissions in AWS Identity and Access Management (IAM) as mentioned below:\n",
    "\n",
    "**`Sagemaker Notebook Execution Role`** \n",
    "Please attach the following policies to your [Amazon SageMaker Notebook IAM Role](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role-sagemaker-notebook.html)\n",
    "* Comprehend Full Access\n",
    "* Sagemaker Full Access\n",
    "* Your Sagemaker Execution Role should have access to S3 already. If not add the following JSON statement as [an inline policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html):\n",
    "    * {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ],\n",
    "            \"Effect\": \"Allow\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "* Add an IAM:PassRole permission as an inline policy to your SageMaker Notebook Execution Role\n",
    "    * {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Action\": [\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Resource\": \"<your-sagemaker-notebook-instance-execution-role-ARN>\"\n",
    "            }\n",
    "           ]\n",
    "        }\n",
    "        \n",
    "**`Trust Relationship for SageMaker execution role`**\n",
    "* Finally [update or replace the Trust Relationship](https://docs.aws.amazon.com/IAM/latest/UserGuide/roles-managingrole-editing-console.html) for your SageMaker execution role with the following JSON statement:\n",
    "    * { \"Version\": \"2012-10-17\", \n",
    "        \"Statement\": [ \n",
    "            { \"Effect\": \"Allow\", \n",
    "              \"Principal\": \n",
    "                { \"Service\": \n",
    "                    [ \"sagemaker.amazonaws.com\", \n",
    "                      \"s3.amazonaws.com\", \n",
    "                      \"comprehend.amazonaws.com\" ] \n",
    "                    }, \n",
    "                    \"Action\": \"sts:AssumeRole\" } \n",
    "                ] \n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Import Libraries\n",
    "\n",
    "We will be using a Textract key value pair example code from the [Amazon Textract documentation](https://docs.aws.amazon.com/textract/latest/dg/examples-extract-kvp.html) for parsing through the Textract response, data science library [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) for content analysis, the [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/), and [AWS boto3 python sdk](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to work with Amazon Textract, Amazon Comprehend and Amazon A2I. Let's now import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser, os\n",
    "import json\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "import uuid\n",
    "import time\n",
    "import io\n",
    "from io import BytesIO\n",
    "import sys\n",
    "import csv\n",
    "from pprint import pprint\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image as PImage, ImageDraw\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))\n",
    "sess = sagemaker.Session()\n",
    "bucket = '<your-s3-bucket>'\n",
    "prefix = 'chapter14'\n",
    "\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Train an Amazon Comprehend Custom Entity Recognizer\n",
    "\n",
    "As a first step we will train an [Amazon Comprehend custom entity recognizer](https://docs.aws.amazon.com/comprehend/latest/dg/custom-entity-recognition.html) model to detect two entities \"PERSON\" or \"GHOST\". A PERSON represents a genuine applicant for a mortgage form, to be sent to downstream applications for further processing and a GHOST represents either a bot or a fake applicant and should be rejected. In this chapter and notebook we will walk you through setting up this workflow in its entirety.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the boto3 handle for comprehend\n",
    "comprehend = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_raw_key = prefix + \"/train/raw_txt.csv\" \n",
    "s3_entity_key = prefix + \"/train/entitylist.csv\"\n",
    "\n",
    "# upload the datasets from our repo to S3\n",
    "s3.upload_file('train/raw_txt.csv',bucket,s3_raw_key)\n",
    "s3.upload_file('train/entitylist.csv',bucket,s3_entity_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 locations for our training inputs\n",
    "\n",
    "s3_raw_txt = 's3://{}/{}'.format(bucket, s3_raw_key)\n",
    "s3_entity_list = 's3://{}/{}'.format(bucket, s3_entity_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a request object to send the S3 location for our entities list and the training dataset\n",
    "cer_input_object = {\n",
    "\n",
    "      \"Documents\": { \n",
    "         \"S3Uri\": s3_raw_txt\n",
    "      },\n",
    "      \"EntityList\": { \n",
    "         \"S3Uri\": s3_entity_list\n",
    "      },\n",
    "      \"EntityTypes\": [\n",
    "                {\n",
    "                    \"Type\": \"PERSON\"\n",
    "                },\n",
    "                {\n",
    "                    \"Type\": \"GHOST\"\n",
    "                }\n",
    "      ]\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "cer_name = \"loan-app-recognizer\"+str(datetime.datetime.now().strftime(\"%s\"))\n",
    "cer_response = comprehend.create_entity_recognizer(\n",
    "        RecognizerName = cer_name, \n",
    "        DataAccessRoleArn = role,\n",
    "        InputDataConfig = cer_input_object,\n",
    "        LanguageCode = \"en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "response = comprehend.describe_entity_recognizer(\n",
    "    EntityRecognizerArn=cer_response['EntityRecognizerArn']\n",
    ")\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the status of training\n",
    "Let us now use the Amazon Comprehend AWS Console to check the status of our training job:\n",
    "1. Go to the [Amazon Comprehend Console](https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#welcome)\n",
    "1. Click on the burger symbol on the top left, and select `custom entity recognition`\n",
    "1. Scroll down a little bit to the `Entity Recognizers` view and click on the name of your Entity recognizer\n",
    "1. Review the `Status`, wait for some time, refresh the page until it changes to `Trained`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create a private human review workforce\n",
    "\n",
    "This step requires you to use the AWS Console. However, we highly recommend that you follow it, especially when creating your own task with a custom template we will use for this notebook. We will create a private workteam and add only one user (you) to it.\n",
    "\n",
    "To create a private team:\n",
    "\n",
    "   1. Go to AWS Console > Amazon SageMaker > Labeling workforces\n",
    "   1. Click \"Private\" and then \"Create private team\".\n",
    "   1. Enter the desired name for your private workteam.\n",
    "   1. Enter your own email address in the \"Email addresses\" section.\n",
    "   1. Enter the name of your organization and a contact email to administer the private workteam.\n",
    "   1. Click \"Create Private Team\".\n",
    "   1. The AWS Console should now return to AWS Console > Amazon SageMaker > Labeling workforces. Your newly created team should be visible under \"Private teams\". Next to it you will see an ARN which is a long string that looks like arn:aws:sagemaker:region-name-123456:workteam/private-crowd/team-name. Please copy this ARN to paste in the cell below.\n",
    "   1. You should get an email from no-reply@verificationemail.com that contains your workforce username and password.\n",
    "   1. In AWS Console > Amazon SageMaker > Labeling workforces, click on the URL in Labeling portal sign-in URL. Use the email/password combination from Step 8 to log in (you will be asked to create a new, non-default password).\n",
    "   1. This is your private worker's interface. When we create a verification task in Verify your task using a private team below, your task should appear in this window. You can invite your colleagues to participate in the labeling job by clicking the \"Invite new workers\" button.\n",
    "\n",
    "Please refer to the [Amazon SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management.html) if you need more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enter the Workteam ARN from step 7 above\n",
    "WORKTEAM_ARN= '<your-private-workteam-arn>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Extract input document contents using Amazon Textract\n",
    "\n",
    "We will now review the input sample loan application image included in the repository, and then use Amazon Textract to first extract the image content, specifically we will select the key value pairs or form data that is of interest to our solution, create an inference request CSV file to pass as an input to our Comprehend custom entity recognizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the input document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Document\n",
    "documentName = \"input/sample-loan-application.png\"\n",
    "\n",
    "display(Image(filename=documentName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now load this image into our S3 bucket\n",
    "s3.upload_file(documentName,bucket,prefix+'/'+documentName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Analyze document using the Textract API\n",
    "\n",
    "We will extract the key value pair data from this document to transform and create a request string for inference.  As you can see we are using the Amazon Textract AnalyzeDocument API. This accepts image files (png or jpeg) as an input. \n",
    "\n",
    "To use this example with a PDF file or for processing multiple documents together replace with [StartDocumentAnalysis API](https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html). The job identifier (JobId) is returned. Amazon Textract sends a message to an Amazon Simple Notification Service (Amazon SNS) topic specified in the call. Call GetDocumentAnalysis, and pass the job identifier (JobId) from the initial call to get the results from Textract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textract = boto3.client('textract')\n",
    "    \n",
    "response = textract.analyze_document(Document={'S3Object': {\n",
    "            'Bucket': bucket,\n",
    "            'Name': prefix+'/'+documentName\n",
    "        }}, FeatureTypes=['FORMS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will install the Amazon Textract Response Parser library that will help us parsing the JSON response from Amazon Textract. Let us first install the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install amazon-textract-response-parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now extract the key and value pairs we need for our solution. We will not use the checkbox fields but only those fields with values in them. Also we will filter out the fields that we actually need in the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trp import Document\n",
    "doc = Document(response)\n",
    "df = pd.DataFrame()\n",
    "# Iterate over elements in the document\n",
    "x = 0\n",
    "for page in doc.pages:\n",
    "    for field in page.form.fields:   \n",
    "        if field.key is not None and field.value is not None:\n",
    "            if field.value.text not in ('SELECTED','NOT_SELECTED'):\n",
    "                df.at[x,'key'] = field.key.text\n",
    "                df.at[x,'value'] = field.value.text\n",
    "                x+=1\n",
    "                #print(\"Field: Key: {}, Value: {}\".format(field.key.text, field.value.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract contents for sending to Comprehend CER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the keys we need\n",
    "df = df.loc[df['key'].isin(['Name (First, Middle, Last, Suffix)','Cell Phone','Years','Social Security Number','Country','Date of Birth (mm/dd/yyyy)','TOTAL $'])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the dataframe so we have all we need in a single row\n",
    "df_T = df.T\n",
    "df_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will drop the key row, rename columns and get it ready to create the CSV file\n",
    "df_T.columns = df_T.iloc[0]\n",
    "df_T = df_T.reset_index(drop=True)\n",
    "df_T = df_T.drop([0])\n",
    "df_T = df_T.reset_index(drop=True)\n",
    "df_T = df_T.rename(columns={\"Name (First, Middle, Last, Suffix)\": \"Name\", \"Date of Birth (mm/dd/yyyy)\": \"Date of Birth\"})\n",
    "df_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Detect Entities using Comprehend custom entity recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Comprehend custom entity recognizer Inference request\n",
    "We will now create a request file that will be sent to the Amazon Comprehend CER model to detect our entities we trained it on. This request CSV file comprises of data that we extracted from our input document using Amazon Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets remove unnecessary spaces and a document column\n",
    "df_T.columns = df_T.columns.str.rstrip()\n",
    "df_T['doc'] = 1\n",
    "df_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Comprehend custom entity inference\n",
    "\n",
    "If you recollect, we trained a custom entity recognizer in Step 1 of this notebook to recognize entities from our input document and determine if the contents indicate a real PERSON or a GHOST. Now we will call the Comprehend custom entity recognizer and pass the contents of the document from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the contents of interest from the extracted document\n",
    "for idx, row in df_T.iterrows():\n",
    "        entry = 'Country'+':'+str(row['Country']).strip()+\" \"+'Years'+':'+str(row['Years']).strip()+\" \"+'Cell Phone'+':'+str(row['Cell Phone']).strip()+\" \"+'Name'+':'+str(row['Name']).strip()+\" \"+'Social Security Number'+':'+str(row['Social Security Number']).strip()+\" \"+'TOTAL $'+':'+str(row['TOTAL $']).strip()+\" \"+'Date of Birth'+':'+str(row['Date of Birth']).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets setup an Amazon Comprehend real-time endpoint\n",
    "custom_recognizer_arn=cer_response['EntityRecognizerArn']\n",
    "\n",
    "endpoint_response = comprehend.create_endpoint(\n",
    "    EndpointName='nlp-chapter14-cer-endpoint',\n",
    "    ModelArn=custom_recognizer_arn,\n",
    "    DesiredInferenceUnits=2,\n",
    "    DataAccessRoleArn=role\n",
    ")\n",
    "\n",
    "endpoint_response['EndpointArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Navigate to Amazon Comprehend console, go to custom entity recognition from the left menu, click on your recognizer, and scroll down to verify your real-time endpoint has been created successfully. If the endpoint is not active, the code in the cell below will fail. It may take about 15 minutes for the endpoint to be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Custom Entity Recognition Job\n",
    "response = comprehend.detect_entities(Text=entry,\n",
    "                    LanguageCode='en',\n",
    "                    EndpointArn=endpoint_response['EndpointArn']\n",
    "            )\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the response from Comprehend for Amazon A2I\n",
    "\n",
    "Let's now create a list to be sent to Amazon A2I for building the UI that the human workflow will use to review the predictions our entity recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the results from the detection\n",
    "import json\n",
    "human_loop_input = []\n",
    "data = {}\n",
    "ent = response['Entities']\n",
    "existing_entities = []\n",
    "if ent != None and len(ent) > 0:\n",
    "    for entity in ent:       \n",
    "        current_entity = {}\n",
    "        current_entity['label'] = entity['Type']\n",
    "        current_entity['text'] = entity['Text']\n",
    "        current_entity['startOffset'] = entity['BeginOffset']\n",
    "        current_entity['endOffset'] = entity['EndOffset']\n",
    "        existing_entities.append(current_entity)\n",
    "        \n",
    "    data['ORIGINAL_TEXT'] = entry\n",
    "    data['ENTITIES'] = existing_entities   \n",
    "    human_loop_input.append(data)\n",
    "\n",
    "print(human_loop_input)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Setup and send to Amazon A2I human loop\n",
    "\n",
    "Now that we have the detected entities from our Comprehend custom entity recognizer, it is time to set up a human workflow using the Private Team we created in `Step 2` and send the results to the Amazon A2I human loop for review, and modifications/augmentation as required. Subsequently, we will update the `entitylist.csv` file that we originally used to train our Comprehend custom entity recognizer so we can prepare it for retraining based on the human feedback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "# Amazon SageMaker client\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "\n",
    "# Amazon Augment AI (A2I) client\n",
    "a2i = boto3.client('sagemaker-a2i-runtime')\n",
    "\n",
    "# Flow definition name\n",
    "flowDefinition = 'fd-nlp-chapter14-' + timestamp\n",
    "\n",
    "# Task UI name - this value is unique per account and region. You can also provide your own value here.\n",
    "taskUIName = 'ui-nlp-chapter14-' + timestamp\n",
    "\n",
    "# Flow definition outputs\n",
    "OUTPUT_PATH = f's3://' + bucket + '/' + prefix + '/a2i-results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the human task UI\n",
    "\n",
    "The template in the cell below will be rendered to the human workers whenever the human loop is required. For over 70 pre built UIs, check: https://github.com/aws-samples/amazon-a2i-sample-task-uis. Let's also declare some variables that we need during the next set of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We customized the tabular template for our notebook as below\n",
    "template = r\"\"\"\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "\n",
    "<crowd-entity-annotation\n",
    "        name=\"crowd-entity-annotation\"\n",
    "        header=\"Highlight parts of the text below\"\n",
    "        labels=\"{{ task.input.labels | to_json | escape }}\"\n",
    "        initial-value=\"{{ task.input.initialValue }}\"\n",
    "        text=\"{{ task.input.originalText }}\"\n",
    ">\n",
    "    <full-instructions header=\"Please follow the instructions below\">\n",
    "        <ol>\n",
    "            <li><strong>Read</strong> the text carefully.</li>\n",
    "            <li><strong>Highlight</strong> words, phrases, or sections of the text.</li>\n",
    "            <li><strong>Choose</strong> the label that best matches what you have highlighted.</li>\n",
    "            <li>To <strong>change</strong> a label, choose highlighted text and select a new label.</li>\n",
    "            <li>To <strong>remove</strong> a label from highlighted text, choose the X next to the abbreviated label name on the highlighted text.</li>\n",
    "            <li>You can select all of a previously highlighted text, but not a portion of it.</li>\n",
    "        </ol>\n",
    "    </full-instructions>\n",
    "\n",
    "    <short-instructions>\n",
    "        Highlight and label the custom entities that were not detected by the model\n",
    "    </short-instructions>\n",
    "\n",
    "</crowd-entity-annotation>\n",
    "\n",
    "<script>\n",
    "    document.addEventListener('all-crowd-elements-ready', () => {\n",
    "        document\n",
    "            .querySelector('crowd-entity-annotation')\n",
    "            .shadowRoot\n",
    "            .querySelector('crowd-form')\n",
    "            .form;\n",
    "    });\n",
    "</script>\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task_ui():\n",
    "    '''\n",
    "    Creates a Human Task UI resource.\n",
    "\n",
    "    Returns:\n",
    "    struct: HumanTaskUiArn\n",
    "    '''\n",
    "    response = sagemaker.create_human_task_ui(\n",
    "        HumanTaskUiName=taskUIName,\n",
    "        UiTemplate={'Content': template})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create task UI\n",
    "humanTaskUiResponse = create_task_ui()\n",
    "humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']\n",
    "print(humanTaskUiArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Flow Definition\n",
    "\n",
    "Flow Definitions allow us to specify:\n",
    "\n",
    "* The workforce that your tasks will be sent to.\n",
    "* The instructions that your workforce will receive. This is called a worker task template.\n",
    "* Where your output data will be stored.\n",
    "* This demo is going to use the API, but you can optionally create this workflow definition in the console as well. \n",
    "\n",
    "For more details and instructions, see: https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-create-flow-definition.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_workflow_definition_response = sagemaker.create_flow_definition(\n",
    "        FlowDefinitionName= flowDefinition,\n",
    "        RoleArn= role,\n",
    "        HumanLoopConfig= {\n",
    "            \"WorkteamArn\": WORKTEAM_ARN,\n",
    "            \"HumanTaskUiArn\": humanTaskUiArn,\n",
    "            \"TaskCount\": 1,\n",
    "            \"TaskDescription\": \"Review the contents and correct values as indicated\",\n",
    "            \"TaskTitle\": \"LOAN APPLICATION REVIEW\"\n",
    "        },\n",
    "        OutputConfig={\n",
    "            \"S3OutputPath\" : OUTPUT_PATH\n",
    "        }\n",
    "    )\n",
    "flowDefinitionArn = create_workflow_definition_response['FlowDefinitionArn'] # let's save this ARN for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(60):\n",
    "    describeFlowDefinitionResponse = sagemaker.describe_flow_definition(FlowDefinitionName=flowDefinition)\n",
    "    print(describeFlowDefinitionResponse['FlowDefinitionStatus'])\n",
    "    if (describeFlowDefinitionResponse['FlowDefinitionStatus'] == 'Active'):\n",
    "        print(\"Flow Definition is active\")\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending predictions to Amazon A2I human loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start the human loop\n",
    "human_loops_started = []\n",
    "\n",
    "import json\n",
    "\n",
    "for line in human_loop_input:\n",
    "    humanLoopName = str(uuid.uuid4())\n",
    "    human_loop_in = {}\n",
    "    human_loop_in['labels'] = [{'label': 'PERSON', 'shortDisplayName': 'PER', 'fullDisplayName': 'PERSON'},{'label': 'GHOST', 'shortDisplayName': 'GHO', 'fullDisplayName': 'GHOST'}]\n",
    "    human_loop_in['originalText'] = line['ORIGINAL_TEXT']\n",
    "    human_loop_in['initialValue'] = line['ENTITIES']\n",
    "                \n",
    "        \n",
    "    start_loop_response = a2i.start_human_loop(\n",
    "        HumanLoopName=humanLoopName,\n",
    "        FlowDefinitionArn=flowDefinitionArn,\n",
    "        HumanLoopInput={\n",
    "                \"InputContent\": json.dumps(human_loop_in)\n",
    "            }\n",
    "        )\n",
    "    print(human_loop_in)\n",
    "    human_loops_started.append(humanLoopName)\n",
    "    print(f'Starting human loop with name: {humanLoopName}  \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check status of human loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "a2i_resp = a2i.describe_human_loop(HumanLoopName=humanLoopName)\n",
    "print(f'HumanLoop Name: {humanLoopName}')\n",
    "print(f'HumanLoop Status: {a2i_resp[\"HumanLoopStatus\"]}')\n",
    "print(f'HumanLoop Output Destination: {a2i_resp[\"HumanLoopOutput\"]}')\n",
    "print('\\n')\n",
    "   \n",
    "      \n",
    "if a2i_resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "    completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Review and modify predictions\n",
    "\n",
    "Now we will login the Amazon A2I Task UI to review, change, and re-label the predictions from Amazon Comprehend custom entity recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's login to the worker portal to review the predictions and modify them as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]\n",
    "print(\"Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!\")\n",
    "print('https://' + sagemaker.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check the status of the human loop again to see it completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "resp = a2i.describe_human_loop(HumanLoopName=humanLoopName)\n",
    "print(f'HumanLoop Name: {humanLoopName}')\n",
    "print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "print('\\n')\n",
    "    \n",
    "if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "    completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's review the annotation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for resp in completed_human_loops:\n",
    "    splitted_string = re.split('s3://' + bucket  + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "    output_bucket_key = splitted_string[1]\n",
    "    response = s3.get_object(Bucket=bucket, Key=output_bucket_key)\n",
    "    content = response[\"Body\"].read()\n",
    "    json_output = json.loads(content)\n",
    "    pp.pprint(json_output)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify if new entities are present\n",
    "\n",
    "If our human workflow team updated the entity and labels, we need to retrain our custom entity recognition model to ensure its able to detect the new or updated entities the next time around. In the next set of cells we will update our original entitylist file with the new or changed entity labels, and submit a retraining job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2i_entities = json_output['humanAnswers'][0]['answerContent']['crowd-entity-annotation']['entities']\n",
    "a2i_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_content = json_output['inputContent']\n",
    "original_text = input_content['originalText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_content)\n",
    "print(\"*********************\")\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain='N'\n",
    "el = open('train/entitylist.csv','r').read()\n",
    "for annotated_entity in a2i_entities:\n",
    "    if original_text[annotated_entity['startOffset']:annotated_entity['endOffset']] not in el:\n",
    "        retrain='Y'\n",
    "        word = '\\n'+original_text[annotated_entity['startOffset']:annotated_entity['endOffset']]+','+annotated_entity['label'].upper()\n",
    "        print(\"Updating Entity List with: \" + word)\n",
    "        open('train/entitylist.csv','a').write(word)\n",
    "\n",
    "if retrain == 'Y':\n",
    "    print(\"Entity list updated, model to be retrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Retrain Comprehend Custom Entity Recognizer with updated entities\n",
    "\n",
    "In the previous section we saw that the human loop had identified one new entity and 2 entities that the model detected correctly, but not present in the original entitylist, so all these 3 entities were updated in the entity list. In this step we will retrain a new Amazon Comprehend model using the updated entity list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_raw_key = prefix + \"/train/raw_txt.csv\" \n",
    "s3_entity_key = prefix + \"/train/entitylist.csv\"\n",
    "\n",
    "# upload the datasets from our repo to S3\n",
    "s3.upload_file('train/raw_txt.csv',bucket,s3_raw_key)\n",
    "s3.upload_file('train/entitylist.csv',bucket,s3_entity_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 locations for our training inputs\n",
    "\n",
    "s3_raw_txt = 's3://{}/{}'.format(bucket, s3_raw_key)\n",
    "s3_entity_list = 's3://{}/{}'.format(bucket, s3_entity_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a request object to send the S3 location for our entities list and the training dataset\n",
    "cer_input_object = {\n",
    "\n",
    "      \"Documents\": { \n",
    "         \"S3Uri\": s3_raw_txt\n",
    "      },\n",
    "      \"EntityList\": { \n",
    "         \"S3Uri\": s3_entity_list\n",
    "      },\n",
    "      \"EntityTypes\": [\n",
    "                {\n",
    "                    \"Type\": \"PERSON\"\n",
    "                },\n",
    "                {\n",
    "                    \"Type\": \"GHOST\"\n",
    "                }\n",
    "      ]\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "cer_name = \"retrain-loan-recognizer\"+str(datetime.datetime.now().strftime(\"%s\"))\n",
    "cer_response = comprehend.create_entity_recognizer(\n",
    "        RecognizerName = cer_name, \n",
    "        DataAccessRoleArn = role,\n",
    "        InputDataConfig = cer_input_object,\n",
    "        LanguageCode = \"en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comprehend.describe_entity_recognizer(\n",
    "    EntityRecognizerArn=cer_response['EntityRecognizerArn']\n",
    ")\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To proceed with testing the retrained recognizer, please follow steps 2 to 5 from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Store predictions for downstream processing\n",
    "\n",
    "Now we understand the complete document management workflow, let us now execute the steps needed to persist the results from our entity detection so we can send it to a downstream application. For example, in our case let us assume that an adjudication application requires our decision to determine if they should process the incoming loan application or not. To do this, we will examine the output from Amazon A2I. If the majority of the entities or all entities are of type \"GHOST\", we will send a rejection decision, if the majority are of type \"PERSON\" we send a summary approval, if all of them are \"PERSON\" we will send approval, and if they are evenly distributed we will send a rejection decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the response from A2I\n",
    "a2i_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's conver the dict to list for ease of use\n",
    "labellist = []\n",
    "for i in a2i_entities:\n",
    "    labellist.append(i['label'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the weights and determine the document status\n",
    "from collections import Counter\n",
    "\n",
    "docstatus = ''\n",
    "\n",
    "ghost = float(Counter(labellist)['GHOST'])\n",
    "person = float(Counter(labellist)['PERSON'])\n",
    "\n",
    "if ghost >= len(labellist)*.5:\n",
    "    docstatus = 'REJECT'\n",
    "elif min(len(labellist)*.5, len(labellist)*.8) < person < max(len(labellist)*.5, len(labellist)*.8):\n",
    "    docstatus = 'SUMMARY APPROVE'\n",
    "elif person > len(labellist)*.8:\n",
    "    docstatus = 'APPROVE'\n",
    "\n",
    "print(docstatus)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the DynamoDB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will create a dynamoDB table and upload the results along with the original content from the document\n",
    "# Get the service resource.\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "tablename = \"loan_status-\"+str(uuid.uuid4())\n",
    "# Create the DynamoDB table.\n",
    "table = dynamodb.create_table(\n",
    "    TableName=tablename,\n",
    "    KeySchema=[\n",
    "        {\n",
    "            'AttributeName': 'doc',\n",
    "            'KeyType': 'HASH'\n",
    "        }\n",
    "    ],\n",
    "    AttributeDefinitions=[\n",
    "        {\n",
    "            'AttributeName': 'doc',\n",
    "            'AttributeType': 'N'\n",
    "        },\n",
    "    ],\n",
    "    ProvisionedThroughput={\n",
    "        'ReadCapacityUnits': 5,\n",
    "        'WriteCapacityUnits': 5\n",
    "    }\n",
    ")\n",
    "# Wait until the table exists, this will take a minute or so\n",
    "table.meta.client.get_waiter('table_exists').wait(TableName=tablename)\n",
    "\n",
    "# Print out some data about the table.\n",
    "print(\"Table successfully created. Item count is: \" + str(table.item_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in df_T.iterrows():\n",
    "    table.put_item(\n",
    "       Item={\n",
    "        'doc': row['doc'],\n",
    "        'Country': str(row['Country']) ,\n",
    "        'Years': str(row['Years']),\n",
    "        'Cell Phone': str(row['Cell Phone']),   \n",
    "        'Name': str(row['Name']),\n",
    "        'Social Security Number': str(row['Social Security Number']),\n",
    "        'TOTAL $': str(row['TOTAL $']),\n",
    "        'Date of Birth': str(row['Date of Birth']),\n",
    "        'Document Status': docstatus \n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Items were successfully created in DynamoDB table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check our insert\n",
    "response = table.get_item(\n",
    "    Key={\n",
    "        'doc': 1\n",
    "    }\n",
    ")\n",
    "item = response['Item']\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "And that's it, we are done with our demo. Please refer to the Further Reading section in the book for more example approaches for this use case as well the code sample for building this same solution using AWS Lambda and CloudFormation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
