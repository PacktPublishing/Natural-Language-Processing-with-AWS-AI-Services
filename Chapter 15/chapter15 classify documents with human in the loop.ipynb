{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac646482",
   "metadata": {},
   "source": [
    "# Classifying documents and setting up human in the loop for active learning\n",
    "In this chapter we will walk you through some reference architecture on how you can easily setup a custom classification model using Amazon Comprehend and have a feedback loop setup with Amazon A2I for active learning on your Comprehend custom model. \n",
    "Here is conceptual architectural flow:\n",
    "    \n",
    "### First using below architecture we will show you how to classify documents by setting a custom classification model and then craeting a real time endpoint for testing\n",
    "\n",
    "![alt-text](cha15train.png)\n",
    "\n",
    "### Secondly,Then we will show you how you can use this endpoint with human in the loop to setup model retraining and active learning workflow\n",
    "\n",
    "![alt-text](chapter15retrain.png)\n",
    "\n",
    "You can automate the entire end to end flow using step function and lambda for orchestration.\n",
    "\n",
    "We will walk you through following steps to classify the documents such as pay stubs and bank statments.\n",
    "\n",
    "### Step 1: Setup and upload  sample documents to Amazon S3\n",
    "### Step 2: Extract text from sample documents using Amazon Textract\n",
    "### Step 3: Create Amazon Comprehend Classification training job\n",
    "### Step 4: Create Amazon Comprehend real time endpoints and test a sample document\n",
    "### Step 5: Setting up active learning with comprehend realtime endpoint using human in the loop \n",
    "\n",
    "Lets start with executing below steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba3e06",
   "metadata": {},
   "source": [
    "## Step 1: Setup and upload  sample documents to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import time\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import io\n",
    "from pathlib import Path\n",
    "import botocore\n",
    "import sagemaker\n",
    "import boto3\n",
    "import io\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "# Document\n",
    "from pprint import pprint\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image as PImage, ImageDraw\n",
    "\n",
    "s3=boto3.client('s3')\n",
    "\n",
    "textract = boto3.client('textract')\n",
    "comprehend=boto3.client(\"comprehend\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ec02ac",
   "metadata": {},
   "source": [
    "Enter bucket name to craete S3 Bucket in your account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = \"doc-processing-bucket-MMDD\"\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "os.environ[\"BUCKET\"] = data_bucket\n",
    "os.environ[\"REGION\"] = region\n",
    "\n",
    "#create s3 bucket\n",
    "if region=='us-east-1':\n",
    "    !aws s3api create-bucket --bucket $BUCKET\n",
    "else:\n",
    "    !aws s3api create-bucket --bucket $BUCKET --create-bucket-configuration LocationConstraint=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06763ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload images to S3 bucket:\n",
    "!aws s3 cp documents/train s3://{data_bucket}/train --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd6a0ea",
   "metadata": {},
   "source": [
    "# Below cell defines a function to get s3 bucket items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037fa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_bucket_items(bucket, prefix, start_after):\n",
    "    list_items=[]\n",
    "    \n",
    "    s3=boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    operation_parameters = {'Bucket': bucket,\n",
    "                            'Prefix': prefix,\n",
    "                            'StartAfter':start_after}\n",
    "    page_iterator = paginator.paginate(**operation_parameters)\n",
    "    for page in page_iterator:\n",
    "        for item in page['Contents']:\n",
    "            list_items.append(item['Key'])\n",
    "    names=list(set([os.path.dirname(x)+'/' for x in list_items]))\n",
    "    images=[x for x in list_items if x not in names and '.ipynb_checkpoints' not in x ]\n",
    "    names=[x.replace(prefix,'').strip('/') for x in names if  '.ipynb_checkpoints' not in x]\n",
    "    return list_items, names, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfb327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images=[]\n",
    "\n",
    "train_objects, names, train_images=get_s3_bucket_items(data_bucket, 'train', 'train/') \n",
    "images.append(train_images)\n",
    "\n",
    "\n",
    "if type(images[0]) is list:\n",
    "    images=[item for sublist in images for item in sublist]\n",
    "    \n",
    "names, images[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90befe",
   "metadata": {},
   "source": [
    "# Setting up local directory structure for extarcted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_prefix=os.getcwd()+'/SAMPLE8/WORDS/'\n",
    "box_prefix=os.getcwd()+'/SAMPLE8/BBOX/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d07b0",
   "metadata": {},
   "source": [
    "# Step 2: Extract text from sample documents using Amazon Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a507bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNCTION FOR EXTRACTING TEXT FROM EACH DOCUMENT AND STORING AS .TXT FILE FOR TRAIN LAYOUTLM USING TEXTRACT\n",
    "\n",
    "def textract_store_train_LM(table, bucket=data_bucket):\n",
    "          \n",
    "    try:\n",
    "\n",
    "        response = textract.detect_document_text(\n",
    "                Document={\n",
    "                    'S3Object': {\n",
    "                        'Bucket': bucket,\n",
    "                        'Name': table\n",
    "                    }\n",
    "                })    \n",
    "        a=[]\n",
    "        b=[]\n",
    "                # Print detected text\n",
    "        for item in response[\"Blocks\"]:\n",
    "\n",
    "            if item[\"BlockType\"] == \"WORD\":\n",
    "                a.append(item['Geometry']['BoundingBox'])\n",
    "                b.append(item[\"Text\"])\n",
    "                #print (item[\"Text\"], end=\" \")\n",
    "                #print (item[\"Text\"], end=\" \")\n",
    "        print(word_prefix)\n",
    "        print(os.path.dirname(table))\n",
    "        Path(word_prefix+os.path.dirname(table)).mkdir(parents=True, exist_ok=True)\n",
    "        Path(box_prefix+os.path.dirname(table)).mkdir(parents=True, exist_ok=True)\n",
    "        with open(word_prefix+table+'.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            for item in b:\n",
    "                f.write(item+'\\n')\n",
    "        with open(box_prefix +table+'.txt', 'w', encoding=\"utf-8\") as p:\n",
    "            for item in a:\n",
    "                p.write(str(item)+'\\n')\n",
    "    except Exception as e:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3acabc",
   "metadata": {},
   "source": [
    "# Call the Textract function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e19581",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "pool.map(textract_store_train_LM, [table for table in images ])\n",
    "print(\"--- %s seconds for extracting ---\" % (time.time() - tic))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58570205",
   "metadata": {},
   "source": [
    "# Step 3: Create Amazon Comprehend Classification training job\n",
    "\n",
    "This section deals with the processing of data for training the comprehend model. \n",
    "\n",
    "The code block below maps extracted text file path and reads the text from each file and stores in a dataframe with the corresponding label in a different column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11773593",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lOOPING THRU THE DIRECTORY AND CREATING A DICT TO HOLD EACH TEXTRACT DOC PATH\n",
    "def data_retriever_from_path(path):    \n",
    "    \n",
    "    mapping={}\n",
    "    for i in names:\n",
    "        if os.path.isdir(path+i):\n",
    "            mapping[i] = sorted(os.listdir(path+i))\n",
    "    # label or class or target list\n",
    "    label_compre = []\n",
    "    # text file data list\n",
    "    text_compre = []\n",
    "    # unpacking and iterating through dictionary\n",
    "    for i, j in mapping.items():\n",
    "        # iterating through list of files for each class\n",
    "        for k in j:\n",
    "            # appending labels/class/target\n",
    "            label_compre.append(i)\n",
    "            # reading the file and appending to data list\n",
    "            text_compre.append(open(path+i+\"/\"+k, encoding=\"utf-8\").read().replace('\\n',' '))\n",
    "    return label_compre, text_compre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00d7be",
   "metadata": {},
   "source": [
    " therefore, all datasets are combined into one and fed to comprehend regardless of your s3 bucket structure.\n",
    "The text for each document are saved in a pandas row (one document per row format) with the corresponding class in another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_compre, text_compre=[],[]\n",
    "\n",
    "path=word_prefix+'train/'\n",
    "label_compre_train, text_compre_train=data_retriever_from_path(path)\n",
    "label_compre.append(label_compre_train)\n",
    "text_compre.append(text_compre_train)\n",
    "\n",
    "if type(label_compre[0]) is list:\n",
    "        label_compre=[item for sublist in label_compre for item in sublist]\n",
    "        #print(label_compre)\n",
    "        text_compre=[item for sublist in text_compre for item in sublist]\n",
    "        #print(text_compre)\n",
    "\n",
    "\n",
    "data_compre= pd.DataFrame()\n",
    "data_compre[\"label\"] =label_compre   \n",
    "data_compre[\"document\"] = text_compre\n",
    "data_compre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a731bcd8",
   "metadata": {},
   "source": [
    "### Craeting Training file from extracted text and saving in Amazon S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_compre=io.StringIO()\n",
    "data_compre.to_csv(csv_compre,index=False, header=False)\n",
    "\n",
    "key='comprehend_train_data.csv'  ### change\n",
    "input_bucket=data_bucket        #### change\n",
    "output_bucket= data_bucket        ### change\n",
    "\n",
    "response2 = s3.put_object(\n",
    "        Body=csv_compre.getvalue(),\n",
    "        Bucket=input_bucket,\n",
    "        Key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80510f",
   "metadata": {},
   "source": [
    "## Go to Amazon Comprehend Console https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#classification to craete a custom classification job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63fadc",
   "metadata": {},
   "source": [
    "Once your job is completed move on to next step, This job take 30 minutes to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22847be7",
   "metadata": {},
   "source": [
    "# Step 4: Create Amazon Comprehend real time endpoints and test a sample document\n",
    "https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#endpoints and copy paste the endpoint ARN below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c52a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ARN='enter your comprehend custom classification endpoint ARN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5e61f",
   "metadata": {},
   "source": [
    "Test the endpoint by passing a test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documentName = \"paystubsample.png\"\n",
    "\n",
    "display(Image(filename=documentName))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77faed7",
   "metadata": {},
   "source": [
    "# Extract Text from this sample doc using Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(service_name='textract',\n",
    "         region_name= 'us-east-1',\n",
    "         endpoint_url='https://textract.us-east-1.amazonaws.com')\n",
    "\n",
    "with open(documentName, 'rb') as file:\n",
    "            img_test = file.read()\n",
    "            bytes_test = bytearray(img_test)\n",
    "            print('Image loaded', documentName)\n",
    "\n",
    "    # process using image bytes\n",
    "response = client.detect_document_text(Document={'Bytes': bytes_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d261b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract key values\n",
    "# Iterate over elements in the document\n",
    "from trp import Document\n",
    "\n",
    "\n",
    "doc = Document(response)\n",
    "page_string = ''\n",
    "for page in doc.pages:\n",
    "    # Print lines and words\n",
    "       \n",
    "        for line in page.lines:\n",
    "            #print((line.text))\n",
    "            page_string += str(line.text)+\"\\n\"\n",
    "print(page_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18283528",
   "metadata": {},
   "source": [
    "# Pass this extracted text to Comprehend classification real time endpoint to classify the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comprehend.classify_document(\n",
    "    Text= page_string,\n",
    "    EndpointArn=ENDPOINT_ARN\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66818985",
   "metadata": {},
   "source": [
    "# Step 5: Setting up active learning with comprehend realtime endpoint using human in the loop \n",
    "We have trained a comprehend custom model and created an endpoint for real time inferencing.\n",
    "Now, In this section we will show you how you can setup human in the loop for model retraining and active learning using below\n",
    "architecure\n",
    "\n",
    "![alt-text](chapter15retrain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a9dd7",
   "metadata": {},
   "source": [
    "### Setting up an Amazon A2I human loop\n",
    "\n",
    "In this section, you set up a human review loop for low-confidence detection in Amazon A2I. It includes the following steps:\n",
    "\n",
    "#### Create a Worker Task template.\n",
    "#### Create a Human review workflow.\n",
    "#### Creating and Starting A2I human loop\n",
    "#### Check the human loop status and start labelling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d873da",
   "metadata": {},
   "source": [
    "# Environment SetupÂ¶\n",
    "\n",
    "We need to set up the following data:\n",
    "WORKTEAM_ARN - To create your Private Workteam, visit the instructions here: https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-private.html After you have created your workteam, replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-east-1'\n",
    "WORKTEAM_ARN= \"enter your workteam arn\"\n",
    "BUCKET = data_bucket\n",
    "ENDPOINT_ARN= ENDPOINT_ARN\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "prefix = \"custom-classify\" + str(uuid.uuid1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon SageMaker client\n",
    "sagemaker = boto3.client('sagemaker', REGION)\n",
    "# A2I Runtime client\n",
    "a2i_runtime_client = boto3.client('sagemaker-a2i-runtime', REGION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a69f011",
   "metadata": {},
   "source": [
    "# Create a Worker Task template\n",
    "\n",
    "It is 2 step process:\n",
    "    \n",
    "    1. Select the UI template you want to use For over 70 pre built UIs, check: https://github.com/aws-samples/amazon-a2i-sample-task-uis\n",
    "    \n",
    "    2. Create Task template using create_human_task_ui API or you can do the same thing using the AWS Console.\n",
    "Refer to this blog to follow AWS Console steps:https://aws.amazon.com/blogs/machine-learning/active-learning-workflow-for-amazon-comprehend-custom-classification-part-2/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Select the UI template for custom classification and modify the categories based on your labels\n",
    "template = \"\"\"<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "\n",
    "<crowd-form>\n",
    "    <crowd-classifier-multi-select\n",
    "      name=\"category\"\n",
    "      categories=\"['Bank Statement', 'Pay Stubs']\"\n",
    "      header=\"Select the relevant categories\"\n",
    "    >\n",
    "      <classification-target>\n",
    "        {{ task.input.taskObject }}\n",
    "      </classification-target>\n",
    "      \n",
    "      <full-instructions header=\"Text Categorization Instructions\">\n",
    "        <p><strong>Bank Statement</strong>Related to payments</p>\n",
    "        <p><strong>Pay Stubs</strong>Related to payment</p>\n",
    "      </full-instructions>\n",
    "\n",
    "      <short-instructions>\n",
    "       Choose all categories that are expressed by the text. \n",
    "      </short-instructions>\n",
    "    </crowd-classifier-multi-select>\n",
    "</crowd-form>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b721f6",
   "metadata": {},
   "source": [
    "# Create a worker task template using boto3 API \n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_human_task_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9defbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task_ui():\n",
    "    '''\n",
    "    Creates a Human Task UI resource.\n",
    "\n",
    "    Returns:\n",
    "    struct: HumanTaskUiArn\n",
    "    '''\n",
    "    response = sagemaker.create_human_task_ui(\n",
    "        HumanTaskUiName=taskUIName,\n",
    "        UiTemplate={'Content': template})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task UI name - this value is unique per account and region. You can also provide your own value here.\n",
    "taskUIName = prefix + '-ui' \n",
    "\n",
    "# Create task UI\n",
    "humanTaskUiResponse = create_task_ui()\n",
    "humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']\n",
    "print(humanTaskUiArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904bc9f",
   "metadata": {},
   "source": [
    "# Use an Amazon Augmented AI (Amazon A2I) human review workflow, or flow definition, to specify the following: \n",
    "  \n",
    "\n",
    "     The workforce that your tasks will be sent to.\n",
    "\n",
    "     The instructions that your workforce will receive, which is called a worker task template.\n",
    "\n",
    "     The configuration of your worker tasks, including the number of workers that receive a task and time limits to complete tasks.\n",
    "\n",
    "     Where your output data will be stored.\n",
    "        \n",
    "To create a flow definition using the SageMaker API, you use the CreateFlowDefinition operation\n",
    "\n",
    "This demo is going to use the API, but you can optionally create this workflow definition in the console as well.\n",
    "\n",
    "For more details and instructions, see: https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-create-flow-definition.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda4249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow definition name - this value is unique per account and region. You can also provide your own value here.\n",
    "flowDefinitionName = prefix + '-fd-a2i' \n",
    "\n",
    "create_workflow_definition_response = sagemaker.create_flow_definition(\n",
    "        FlowDefinitionName= flowDefinitionName,\n",
    "        RoleArn= role,\n",
    "        HumanLoopConfig= {\n",
    "            \"WorkteamArn\": WORKTEAM_ARN,\n",
    "            \"HumanTaskUiArn\": humanTaskUiArn,\n",
    "            \"TaskCount\": 1,\n",
    "            \"TaskDescription\": \"Read the instructions\",\n",
    "            \"TaskTitle\": \"Classify the text\"\n",
    "        },\n",
    "        OutputConfig={\n",
    "            \"S3OutputPath\" : \"s3://\"+BUCKET+\"/output\"\n",
    "        }\n",
    "    )\n",
    "flowDefinitionArn = create_workflow_definition_response['FlowDefinitionArn'] # let's save this ARN for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72910c0",
   "metadata": {},
   "source": [
    "# Sample Data to Test Comprehend Endpoint and create a request for A2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comprehend.classify_document(\n",
    "    Text= page_string,\n",
    "    EndpointArn=ENDPOINT_ARN\n",
    ")\n",
    "print(response)\n",
    "p = response['Classes'][0]['Name']\n",
    "score = response['Classes'][0]['Score']\n",
    "        #print(f\"S:{sentence}, Score:{score}\")\n",
    "response = {}\n",
    "response['utterance']=page_string\n",
    "response['prediction']=p\n",
    "response['confidence'] = score\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a674b",
   "metadata": {},
   "source": [
    "# Creating and Starting A2I human loop\n",
    "\n",
    "For more information https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-start-human-loop.html#a2i-instructions-starthumanloop\n",
    "\n",
    "When using Amazon A2I for a custom task, a human loops starts when StartHumanLoop is called in your application. Prerequisites\n",
    "\n",
    "To complete this procedure, you need:\n",
    "\n",
    "Input data formatted as a string representation of a JSON-formatted file.\n",
    "\n",
    "The Amazon Resource Name (ARN) of your flow definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "human_loops_started = []\n",
    "CONFIDENCE_SCORE_THRESHOLD = .90\n",
    "if(response['confidence'] > CONFIDENCE_SCORE_THRESHOLD):\n",
    "        humanLoopName = str(uuid.uuid4())\n",
    "        human_loop_input = {}\n",
    "  \n",
    "        human_loop_input['taskObject'] = response['utterance']\n",
    "        start_loop_response = a2i_runtime_client.start_human_loop(\n",
    "        HumanLoopName=humanLoopName,\n",
    "        FlowDefinitionArn=flowDefinitionArn,\n",
    "        HumanLoopInput={\n",
    "                \"InputContent\": json.dumps(human_loop_input)\n",
    "            }\n",
    "        )\n",
    "        print(human_loop_input)\n",
    "        human_loops_started.append(humanLoopName)\n",
    "        print(f'Score is less than the threshold of {CONFIDENCE_SCORE_THRESHOLD}')\n",
    "        print(f'Starting human loop with name: {humanLoopName}  \\n')\n",
    "else:\n",
    "         print('No human loop created. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130c83ff",
   "metadata": {},
   "source": [
    "# Navigate to the private worker portal and start Labelling!\n",
    "\n",
    "Make sure you've invited yourself to your workteam!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]\n",
    "print(\"Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!\")\n",
    "print('https://' + sagemaker.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "resp = a2i_runtime_client.describe_human_loop(HumanLoopName=humanLoopName)\n",
    "print(f'HumanLoop Name: {humanLoopName}')\n",
    "print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "#print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "print('\\n')\n",
    "    \n",
    "if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "    completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebfdfc6",
   "metadata": {},
   "source": [
    "# Review the labelling results in Amazon S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1edb768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for resp in completed_human_loops:\n",
    "    splitted_string = re.split('s3://' + data_bucket  + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "    output_bucket_key = splitted_string[1]\n",
    "    print(output_bucket_key)\n",
    "    response = s3.get_object(Bucket=data_bucket, Key=output_bucket_key)\n",
    "    print(data_bucket)\n",
    "    content = response[\"Body\"].read()\n",
    "    json_output = json.loads(content)\n",
    "    pp.pprint(json_output)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0acde",
   "metadata": {},
   "source": [
    "# Combining this augmented data for retraining with original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New values\n",
    "for i in json_output['humanAnswers']:\n",
    "    x = i['answerContent']\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a8ffc",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19d08f",
   "metadata": {},
   "source": [
    "# Deleteing the model endpoint and Comprehend training jobs in your account.\n",
    "#Run below code to delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comprehend.delete_endpoint(\n",
    "    EndpointArn=ENDPOINT_ARN\n",
    ")\n",
    "\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf28864b",
   "metadata": {},
   "source": [
    "# Go to AWS Console and delete the S3 bucket, Comprehend the model training jobs and workflow definition\n",
    "\n",
    "https://docs.aws.amazon.com/AmazonS3/latest/userguide/delete-bucket.html\n",
    "    \n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-delete-flow-definition.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
