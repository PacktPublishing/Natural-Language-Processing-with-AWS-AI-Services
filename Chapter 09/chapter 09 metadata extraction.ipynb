{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install amazon-textract-response-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import smart_open\n",
    "\n",
    "from time import sleep\n",
    "from matplotlib import cm, colors\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from pyvis.network import Network\n",
    "from trp import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon Textract client\n",
    "textract = boto3.client('textract')\n",
    "#amazon comprehend\n",
    "comprehend_client = boto3.client('comprehend')\n",
    "# Client and session information\n",
    "session = boto3.Session()\n",
    "s3_client = session.client(service_name=\"s3\")\n",
    "# Amazon S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter your Amazon S3 bucket name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for S3 bucket and input data file\n",
    "bucket = \"<enter your s3 bucket name>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download sample financial documents to S3\n",
    "\n",
    "We've included a set of Amazon press releases as example documents. Here we upload them as a single file `sample_financial_news_doc.pdf` to an S3 bucket for processing. The same bucket will be used to return service output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sample_financial_news_doc.pdf\"\n",
    "# Upload the local file to S3\n",
    "s3_client.upload_file(filename, bucket, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document name in Amazon S3 Bucket\n",
    "documentName = bucket + '/'+filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert pdf documents to text using Amazon Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startJob(s3BucketName, objectName):\n",
    "    response = None\n",
    "    response = textract.start_document_text_detection(\n",
    "    DocumentLocation={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': objectName\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return response[\"JobId\"]\n",
    "\n",
    "def isJobComplete(jobId):\n",
    "    response = textract.get_document_text_detection(JobId=jobId)\n",
    "    status = response[\"JobStatus\"]\n",
    "    print(\"Job status: {}\".format(status))\n",
    "\n",
    "    while(status == \"IN_PROGRESS\"):\n",
    "        time.sleep(5)\n",
    "        response = textract.get_document_text_detection(JobId=jobId)\n",
    "        status = response[\"JobStatus\"]\n",
    "        print(\"Job status: {}\".format(status))\n",
    "\n",
    "    return status\n",
    "\n",
    "def getJobResults(jobId):\n",
    "\n",
    "    pages = []\n",
    "    response = textract.get_document_text_detection(JobId=jobId)\n",
    "    \n",
    "    pages.append(response)\n",
    "    print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "    nextToken = None\n",
    "    if('NextToken' in response):\n",
    "        nextToken = response['NextToken']\n",
    "\n",
    "    while(nextToken):\n",
    "        response = textract.get_document_text_detection(JobId=jobId, NextToken=nextToken)\n",
    "\n",
    "        pages.append(response)\n",
    "        print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "        nextToken = None\n",
    "        if('NextToken' in response):\n",
    "            nextToken = response['NextToken']\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobId = startJob(bucket, filename)\n",
    "print(\"Started job with id: {}\".format(jobId))\n",
    "if(isJobComplete(jobId)):\n",
    "    response = getJobResults(jobId)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the extracted data from Amazon Textract into UTF 8 Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the  data into a text file\n",
    "text_filename = 'sample_finance_data.txt'\n",
    "doc = Document(response)\n",
    "with open(text_filename, 'w', encoding='utf-8') as f:\n",
    "    for page in doc.pages:\n",
    "    # Print lines and words\n",
    "        page_string = ''\n",
    "        for line in page.lines:\n",
    "            #print((line.text))\n",
    "            page_string += str(line.text)\n",
    "        #print(page_string)\n",
    "        f.writelines(page_string + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents locally for later analysis\n",
    "with open(text_filename, \"r\") as fi:\n",
    "    raw_texts = [line.strip() for line in fi.readlines()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload this text file to Amazon S3 for Comprehend events analysis jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the local file to S3\n",
    "s3_client.upload_file(text_filename, bucket, text_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Extraction\n",
    "With Comprehend entity detection\n",
    "and with Comprehend Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets extract some metadata using Amazon Comprehend Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two choices here\n",
    "1. Create Comprehend events analysis job through console OR\n",
    "2. Start an asnynchronous job with python SDK by running below notebook cell\n",
    "\n",
    "If you want to follow the steps using AWS Console click here https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#home\n",
    "and follow instrcutions in the chapter 9\n",
    "\n",
    "Note: If you are craeting events using Amazon Comprehend console, skip the \"Start an asynchronous job with the SDK\" section and move to \"Collect the results from S3\" Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start an asynchronous job with the SDK\n",
    "\n",
    "The first task is to kick off the inference job. We'll do this with the `start_events_detection_job` endpoint. Note that the API requires an IAM role with List, Read, and Write access to the bucket specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_data_s3_path = f's3://{bucket}/' + text_filename\n",
    "output_data_s3_path = f's3://{bucket}/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a IAM role with Access to Comprehend and specified s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IAM role with access to Comprehend and specified S3 buckets\n",
    "job_data_access_role = '<enter iam role or refer to code in action video>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other job parameters\n",
    "input_data_format = 'ONE_DOC_PER_LINE'\n",
    "job_uuid = uuid.uuid1()\n",
    "job_name = f\"events-job-{job_uuid}\"\n",
    "event_types = [\"BANKRUPTCY\", \"EMPLOYMENT\", \"CORPORATE_ACQUISITION\", \n",
    "               \"INVESTMENT_GENERAL\", \"CORPORATE_MERGER\", \"IPO\",\n",
    "               \"RIGHTS_ISSUE\", \"SECONDARY_OFFERING\", \"SHELF_OFFERING\",\n",
    "               \"TENDER_OFFERING\", \"STOCK_SPLIT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the inference job\n",
    "response = comprehend_client.start_events_detection_job(\n",
    "    InputDataConfig={'S3Uri': input_data_s3_path,\n",
    "                     'InputFormat': input_data_format},\n",
    "    OutputDataConfig={'S3Uri': output_data_s3_path},\n",
    "    DataAccessRoleArn=job_data_access_role,\n",
    "    JobName=job_name,\n",
    "    LanguageCode='en',\n",
    "    TargetEventTypes=event_types\n",
    ")\n",
    "\n",
    "# Get the job ID\n",
    "events_job_id = response['JobId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above code will submit a job in Comprehend Analysis job.\n",
    "Go to Amazon Console to get the Job Id once the job is completed.\n",
    "https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#analysis\n",
    "\n",
    " Note that, as an asynchronous inference job, the task will take several minutes to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you have created events job using Comprehend console, go to the analysis job and copy the job id and paste it below else continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment and enter job id to run this after job is completed\n",
    "events_job_id =\"<enter completed analysis job id>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current job status\n",
    "job = comprehend_client.describe_events_detection_job(JobId=events_job_id)\n",
    "\n",
    "# Loop until job is completed\n",
    "waited = 0\n",
    "timeout_minutes = 30\n",
    "while job['EventsDetectionJobProperties']['JobStatus'] != 'COMPLETED':\n",
    "    sleep(60)\n",
    "    waited += 60\n",
    "    assert waited//60 < timeout_minutes, \"Job timed out after %d seconds.\" % waited\n",
    "    job = comprehend_client.describe_events_detection_job(JobId=events_job_id)\n",
    "    print(\"Job Status {}\".format(job['EventsDetectionJobProperties']['JobStatus']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output filename is the input filename + \".out\"\n",
    "output_data_s3_file = job['EventsDetectionJobProperties']['OutputDataConfig']['S3Uri'] + text_filename + '.out'\n",
    "print(output_data_s3_file)\n",
    "# Load the output into a result dictionary    # Get the files.\n",
    "results = []\n",
    "with smart_open.open(output_data_s3_file) as fi:\n",
    "    results.extend([json.loads(line) for line in fi.readlines() if line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analyzing Comprehend Events output\n",
    "\n",
    "The remainder of this notebook provides examples of different ways to analyze a given document. For our example document, we'll use the kind of online posting that a Financial analyst might consume when projecting market trends, a [2017 press release about Amazon's acquisition of Whole Foods Market, Inc.](https://press.aboutamazon.com/news-releases/news-release-details/amazoncom-announces-third-quarter-sales-34-437-billion). It's the first document in the data set we submitted to the Comprehend Events API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Comprehend Events system output\n",
    "\n",
    "The system returns JSON output for each submitted document. The structure of a response is shown below. Note:\n",
    "\n",
    "* Events system output contains separate objects for `Entities` and `Events`, each organized into groups of coreferential object.  \n",
    "* Two additional fields, `File` and `Line` will be present as well to track document provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first result document for analysis\n",
    "result = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Events are groups of Triggers\n",
    "\n",
    "* The API output includes the text, character offset, and type of each trigger.  \n",
    "\n",
    "* Confidence scores for classification tasks are given as `Score`. Confidence of event group membership is given with `GroupScore`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "result['Events'][1]['Triggers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Arguments are linked to Entities by EntityIndex\n",
    "\n",
    "* The API also return the classification confidence of the role assignment.\n",
    "It talks about how the entity is related to the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "result['Events'][1]['Arguments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Entities are groups of Mentions\n",
    "\n",
    "* The API output includes the text, character offset, and type of each mention.  \n",
    "\n",
    "* Confidence scores for classification tasks are given as `Score`. Confidence of entity group membership is given with `GroupScore`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "result['Entities'][5]['Mentions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the Events and Entities\n",
    "\n",
    "In the remainder of the notebook, we'll give a number of tabulations and visualizations to help understand what the API is returning.\n",
    "\n",
    "First we'll consider visualization of spans, both triggers and entity mentions. One of the most essential visualization tasks for sequence labeling tasks is highlighting of tagged text in documents. For demo purposes, we'll do this with [displaCy](https://spacy.io/usage/visualizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Convert Events output to displaCy format.\n",
    "entities = [\n",
    "    {'start': m['BeginOffset'], 'end': m['EndOffset'], 'label': m['Type']}\n",
    "    for e in result['Entities']\n",
    "    for m in e['Mentions']\n",
    "]\n",
    "\n",
    "triggers = [\n",
    "    {'start': t['BeginOffset'], 'end': t['EndOffset'], 'label': t['Type']}\n",
    "    for e in result['Events']\n",
    "    for t in e['Triggers']\n",
    "]\n",
    "\n",
    "# Spans need to be sorted for displaCy to process them correctly\n",
    "spans = sorted(entities + triggers, key=lambda x: x['start'])\n",
    "tags = [s['label'] for s in spans]\n",
    "\n",
    "output = [{\"text\": raw_texts[0], \"ents\": spans, \"title\": None, \"settings\": {}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Misc. objects for presentation purposes\n",
    "spectral = cm.get_cmap(\"Spectral\", len(tags))\n",
    "tag_colors = [colors.rgb2hex(spectral(i)) for i in range(len(tags))]\n",
    "color_map = dict(zip(*(tags, tag_colors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Note that only Entities participating in Events are shown.\n",
    "displacy.render(output, style=\"ent\", options={\"colors\": color_map}, manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rendering as tabular data\n",
    "\n",
    "Many users will use Events to create structured data from unstructured text. Here we'll demonstrate how to do this with `pandas`. First, we flatten hierarchical JSON to pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Creation of the entity dataframe. Entity indices must be explicitly created.\n",
    "entities_df = pd.DataFrame([\n",
    "    {\"EntityIndex\": i, **m}\n",
    "    for i, e in enumerate(result['Entities'])\n",
    "    for m in e['Mentions']\n",
    "])\n",
    "\n",
    "# Creation of the events dataframe. Event indices must be explicitly created.\n",
    "events_df = pd.DataFrame([\n",
    "    {\"EventIndex\": i, **a, **t}\n",
    "    for i, e in enumerate(result['Events'])\n",
    "    for a in e['Arguments']\n",
    "    for t in e['Triggers']\n",
    "])\n",
    "\n",
    "# Join the two tables into one flat data structure.\n",
    "events_df = events_df.merge(entities_df, on=\"EntityIndex\", suffixes=('Event', 'Entity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A more succinct representation\n",
    "\n",
    "We're primarity interested in the *event structure*, so let's make that more transparent by creating a new table with Roles as column headers, grouped by Event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def format_compact_events(x):\n",
    "    \"\"\"Collapse groups of mentions and triggers into a single set.\"\"\"\n",
    "    # Take the most commonly occurring EventType and the set of triggers.\n",
    "    d = {\"EventType\": Counter(x['TypeEvent']).most_common()[0][0],\n",
    "         \"Triggers\": set(x['TextEvent'])}\n",
    "    # For each argument Role, collect the set of mentions in the group.\n",
    "    for role in x['Role']:\n",
    "        d.update({role: set((x[x['Role']==role]['TextEntity']))})\n",
    "    return d\n",
    "\n",
    "# Group data by EventIndex and format.\n",
    "event_analysis_df = pd.DataFrame(\n",
    "    events_df.groupby(\"EventIndex\").apply(format_compact_events).tolist()\n",
    ").fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "event_analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Graphing event semantics\n",
    "\n",
    "The most striking representation of Comprehend Events output is found in a semantic graph, a network of the entities and events referenced in a document or documents. The code below uses two open source libraries, `networkx` and `pyvis`, to render events system output. In the resulting graph, nodes are entity mentions and triggers, while edges are the argument roles held by the entities in relation to the triggers.\n",
    "In the graph, vertices are entity mentions and triggers; edges are the argument roles held by the entities in relation to the triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Formatting the data\n",
    "\n",
    "System output must first be conformed to the node (i.e., vertex) and edge list format required by `networkx`. This requires iterating over triggers, entities, and argument structural relations. Note that we can use the `GroupScore` and `Score` keys on various objects to prune nodes and edges in which the model has less confidence. We can also use various strategies to pick a 'canonical' mention from each mention group to appear in the graph; here we chose the mention with the string-wise longest extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Entities are associated with events by group, not individual mention; for simplicity, \n",
    "# assume the canonical mention is the longest one.\n",
    "def get_canonical_mention(mentions):\n",
    "    extents = enumerate([m['Text'] for m in mentions])\n",
    "    longest_name = sorted(extents, key=lambda x: len(x[1]))\n",
    "    return [mentions[longest_name[-1][0]]]\n",
    "\n",
    "# Set a global confidence threshold\n",
    "thr = 0.5\n",
    "\n",
    "# Nodes are (id, type, tag, score, mention_type) tuples.\n",
    "trigger_nodes = [\n",
    "    (\"tr%d\" % i, t['Type'], t['Text'], t['Score'], \"trigger\")\n",
    "    for i, e in enumerate(result['Events'])\n",
    "    for t in e['Triggers'][:1]\n",
    "    if t['GroupScore'] > thr\n",
    "]\n",
    "entity_nodes = [\n",
    "    (\"en%d\" % i, m['Type'], m['Text'], m['Score'], \"entity\")\n",
    "    for i, e in enumerate(result['Entities'])\n",
    "    for m in get_canonical_mention(e['Mentions'])\n",
    "    if m['GroupScore'] > thr\n",
    "]\n",
    "\n",
    "# Edges are (trigger_id, node_id, role, score) tuples.\n",
    "argument_edges = [\n",
    "    (\"tr%d\" % i, \"en%d\" % a['EntityIndex'], a['Role'], a['Score'])\n",
    "    for i, e in enumerate(result['Events'])\n",
    "    for a in e['Arguments']\n",
    "    if a['Score'] > thr\n",
    "]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Create a compact graph\n",
    "\n",
    "Once the nodes and edges are defines, we can create and visualize the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "# Iterate over triggers and entity mentions.\n",
    "for mention_id, tag, extent, score, mtype in trigger_nodes + entity_nodes:\n",
    "    label = extent if mtype.startswith(\"entity\") else tag\n",
    "    G.add_node(mention_id, label=label, size=score*10, color=color_map[tag], tag=tag, group=mtype)\n",
    "    \n",
    "# Iterate over argument role assignments\n",
    "for event_id, entity_id, role, score in argument_edges:\n",
    "    G.add_edges_from(\n",
    "        [(event_id, entity_id)],\n",
    "        label=role,\n",
    "        weight=score*100,\n",
    "        color=\"grey\"\n",
    "    )\n",
    "\n",
    "# Drop mentions that don't participate in events\n",
    "G.remove_nodes_from(list(nx.isolates(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "nt = Network(\"600px\", \"800px\", notebook=True, heading=\"\")\n",
    "nt.from_nx(G)\n",
    "nt.show(\"compact_nx.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A more complete graph\n",
    "\n",
    "The graph above is compact, only relaying essential event type and argument role information. We can use a slightly more complicated set of functions to graph all of the information returned by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This convenience function in `events_graph.py` plots a complete graph of the document,\n",
    "# showing all events, triggers, entities, and their groups.\n",
    "\n",
    "import events_graph as evg\n",
    "\n",
    "evg.plot(result, node_types=['event', 'trigger', 'entity_group', 'entity'], thr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete Amazon S3 Bucket and objects in the bucket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
