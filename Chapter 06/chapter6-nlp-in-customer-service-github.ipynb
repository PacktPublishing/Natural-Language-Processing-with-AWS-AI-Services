{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hundred-heather",
   "metadata": {},
   "source": [
    "# Using NLP to improve Custom Service operations\n",
    "\n",
    "This is the accompanying notebook for Chapter 6 in the book - Natural Language Processing with AWS AI Services. Please ensure you have read and followed the instructions in Chapter 6 before trying out the steps in this notebook. Briefly this notebook covers the code required for the following topics covered:\n",
    "\n",
    "1. Prerequisites\n",
    "1. Preprocess the Text Data\n",
    "1. Process Topic Modeling Results\n",
    "1. Train an Amazon Comprehend Custom Classifier\n",
    "1. Automate Request Routing using the Classifier\n",
    "1. Automate feedback analysis using Comprehend Sentiment Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-playlist",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latest-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser, os\n",
    "import json\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "import uuid\n",
    "import time\n",
    "import io\n",
    "from io import BytesIO\n",
    "import sys\n",
    "import csv\n",
    "from pprint import pprint\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image as PImage, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attached-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = '<your-s3-bucket>'\n",
    "prefix = 'chapter6'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-young",
   "metadata": {},
   "source": [
    "## Preprocess the Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-uruguay",
   "metadata": {},
   "source": [
    "We will use the Consumer Complaints data for the State of Ohio from the Consumer Financial Protection Bureau for our solution - https://www.consumerfinance.gov/data-research/consumer-complaints/search/?dataNormalization=None&dateRange=1y&date_received_max=2021-05-17&date_received_min=2020-05-17&searchField=all&state=OH&tab=Map. You can try other datasets from this site, or your own unique customer service data. For your convenience, I have included the complaints data as a CSV file in the github repository, which should be available to you when you clone the repo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file into a Pandas DataFrame for easy manipulation\n",
    "raw_df = pd.read_csv('topic-modeling/initial/complaints_data_initial.csv')\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the rows where the complaint field is empty\n",
    "raw_df = raw_df.dropna(subset=['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the rest of the columns, we only need the complaint field for our solution\n",
    "raw_df = pd.DataFrame(raw_df['Consumer complaint narrative'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this back to the CSV file\n",
    "directory = \"raw\"\n",
    "parent_dir = os.getcwd()+'/topic-modeling'\n",
    " \n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "os.makedirs(path, exist_ok = True)\n",
    "print(\"Directory '%s' created successfully\" %directory)\n",
    "\n",
    "raw_df.to_csv('topic-modeling/raw/complaints_data_subset.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex expression method for splitting the text into individual sentences\n",
    "# source - https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(r'(?<=\\d)[\\.](?=\\d)','',text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\". \",\"\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    #text = text.replace(\"?\",\"\")\n",
    "    #text = text.replace(\"!\",\"\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    text = text.replace('\"','')\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Regex expression to create a list of sentences\n",
    "folderpath = r\"topic-modeling/raw\" # make sure to put the 'r' in front and provide the folder where your files are\n",
    "filepaths  = [os.path.join(folderpath, name) for name in os.listdir(folderpath) if not name.startswith('.')] # do not select hidden directories\n",
    "print(filepaths)\n",
    "all_files = []\n",
    "\n",
    "for path in filepaths:\n",
    "    with open(path, 'r') as f:\n",
    "        structured_text = split_into_sentences(f.read())\n",
    "        all_files.append(structured_text)        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the formatted sentences into a CSV file\n",
    "import csv\n",
    "fnfull = \"topic-modeling/input/complaints_data_formatted.csv\"\n",
    "directory = \"input\"\n",
    "parent_dir = os.getcwd()+'/topic-modeling'\n",
    "\n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "os.makedirs(path, exist_ok = True)\n",
    "print(\"Directory '%s' created successfully\" %directory)\n",
    "\n",
    "with open(fnfull, \"w\", encoding='utf-8') as ff:\n",
    "    csv_writer = csv.writer(ff, delimiter=',', quotechar = '\"')\n",
    "    for infile in all_files:\n",
    "        for num, sentence in enumerate(infile):\n",
    "            csv_writer.writerow([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's store the formatted CSV into a Pandas DataFrame \n",
    "# as we will use this to create the training dataset for our custom classifier\n",
    "columns = ['Text']\n",
    "form_df = pd.read_csv('topic-modeling/input/complaints_data_formatted.csv', header=None, names = columns)\n",
    "form_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the CSV file to the input prefix in S3 to be used in the topic modeling job\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file('topic-modeling/input/complaints_data_formatted.csv', bucket, prefix+'/topic_modeling/input/topic_input.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-israel",
   "metadata": {},
   "source": [
    "#### Now follow the instructions in the book to run the topic modeling job from the Amazon Comprehend console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-above",
   "metadata": {},
   "source": [
    "## Process Topic Modeling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first download the results of the topic modeling job. \n",
    "# Please copy the output data location from your topic modeling job for this step and use it below\n",
    "\n",
    "directory = \"results\"\n",
    "parent_dir = os.getcwd()+'/topic-modeling'\n",
    " \n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "os.makedirs(path, exist_ok = True)\n",
    "print(\"Directory '%s' created successfully\" %directory)\n",
    "\n",
    "tpprefix = prefix+'/topic_modeling/results/<name-of-your-output-data-s3-prefix>/output/output.tar.gz'\n",
    "s3.download_file(bucket, tpprefix, 'topic-modeling/results/output.tar.gz')\n",
    "!tar -xzvf topic-modeling/results/output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load each of the resulting CSV files to their own DataFrames\n",
    "tt_df = pd.read_csv('topic-terms.csv')\n",
    "dt_df = pd.read_csv('doc-topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the topic terms DataFrame contains the topic number, what term corresponds to the topic, and \n",
    "# the weightage of this term contributing to the topic\n",
    "for i,x in tt_df.iterrows():\n",
    "    print(str(x['topic'])+\":\"+x['term']+\":\"+str(x['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may have multiple topics in the same line, but for this example we are not interested in these duplicates, so we will drop it\n",
    "dt_df = dt_df.drop_duplicates(subset=['docname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows in the mean range of weightage for a topic\n",
    "ttdf_max = tt_df.groupby(['topic'], sort=False)['weight'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load these into its own DataFrame and remove terms that are masked\n",
    "newtt_df = pd.DataFrame()\n",
    "for x in ttdf_max:\n",
    "    newtt_df = newtt_df.append(tt_df.query('weight == @x'))\n",
    "\n",
    "newtt_df = newtt_df.reset_index(drop=True)    \n",
    "newtt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having review the input document, the masked characters mainly correspond to debt related complaints from customers\n",
    "# so we will replace the masked terms with Debt and we will replace the word Husband with Family\n",
    "\n",
    "form_df.assign(Label='')\n",
    "\n",
    "for i, r in dt_df.iterrows():\n",
    "    line = int(r['docname'].split(':')[1])\n",
    "    top = r['topic']\n",
    "    tdf = newtt_df.query('topic == @top')\n",
    "    term = tdf['term'].values[0]\n",
    "    if term == 'xxxx':\n",
    "        term = 'debt'\n",
    "    if term == 'husband':\n",
    "        term = 'family'\n",
    "    form_df.at[line, 'Label'] = term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "form_df['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the custom-classification directory\n",
    "directory = \"custom-classification\"\n",
    "parent_dir = os.getcwd()\n",
    " \n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "os.makedirs(path, exist_ok = True)\n",
    "print(\"Directory '%s' created successfully\" %directory)\n",
    "\n",
    "# create the train directory\n",
    "directory = \"train\"\n",
    "parent_dir = os.getcwd()+'/custom-classification'\n",
    " \n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "os.makedirs(path, exist_ok = True)\n",
    "print(\"Directory '%s' created successfully\" %directory)\n",
    "\n",
    "# Let's now rearrange the columns to have the label as the first column\n",
    "form_df = form_df[['Label','Text']]\n",
    "form_df.to_csv('custom-classification/train/train.csv', header=None, index=False)\n",
    "s3.upload_file('custom-classification/train/train.csv', bucket, prefix+'/custom_classification/train/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-syndrome",
   "metadata": {},
   "source": [
    "#### Now follow the instructions in the book to train your Amazon Comprehend Custom Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-porter",
   "metadata": {},
   "source": [
    "## Automate Request Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_arn = '<comprehend-custom-classifier-endpoint-arn>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use the our custom classifier for real-time analysis\n",
    "test_text = 'because of your inability to accept my payments on time I now have a really bad credit score, you need to fix this now'\n",
    "comprehend = boto3.client('comprehend')\n",
    "response = comprehend.classify_document(Text=test_text, EndpointArn=endpoint_arn)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the label name for the maximum score which is where this request should be routed to\n",
    "cls_df = pd.DataFrame(response['Classes'])\n",
    "max_score = cls_df['Score'].max()\n",
    "routing_type = cls_df.query('Score == @max_score')['Name'].values[0]\n",
    "print(\"This request should be routed to: \" + routing_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-singing",
   "metadata": {},
   "source": [
    "## Automate Feeback Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use Amazon Comprehend Detect Sentiment API to analyze the customer's feedback\n",
    "sent_response = comprehend.detect_sentiment(\n",
    "    Text=test_text,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "print(\"The customer's feedback sentiment is: \" + sent_response['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-thomas",
   "metadata": {},
   "source": [
    "#### This concludes the notebook, please go back to the book for reviewing the next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-sunset",
   "metadata": {},
   "source": [
    "### End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
